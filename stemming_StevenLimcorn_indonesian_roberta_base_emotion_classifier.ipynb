{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stemming_StevenLimcorn/indonesian-roberta-base-emotion-classifier",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JZH-j7WzxCS",
        "outputId": "9c309758-02ef-4c53-cee5-de48580d57c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('data_cleanbanget.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DNzbIVS1p5L-",
        "outputId": "ad887ffe-e950-4590-e4d2-ba7c12aea85e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               tweet  label\n",
              "0  jln jatibarupolisi tdk bs gertak gubernur eman...  anger\n",
              "1  cewe lho kayak rasain sibuk jaga rasain sakit ...  anger\n",
              "2  kepingin gudeg mbarek bu hj amad foto google s...  happy\n",
              "3  jln jatibarubagian wilayah tn abangpengaturan ...  anger\n",
              "4  sharing alam aja kemarin jam 1800 batalin tike...  happy"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0a64529c-74a4-4a30-9d0d-30564b074c21\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jln jatibarupolisi tdk bs gertak gubernur eman...</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cewe lho kayak rasain sibuk jaga rasain sakit ...</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kepingin gudeg mbarek bu hj amad foto google s...</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jln jatibarubagian wilayah tn abangpengaturan ...</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sharing alam aja kemarin jam 1800 batalin tike...</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a64529c-74a4-4a30-9d0d-30564b074c21')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0a64529c-74a4-4a30-9d0d-30564b074c21 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0a64529c-74a4-4a30-9d0d-30564b074c21');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaI6AfHMpjhn",
        "outputId": "49a85f5c-8c91-4f1c-bb3e-8e0568c288ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['label'] == 'anger', 'labels'] = 1\n",
        "df.loc[df['label'] =='happy', 'labels'] = 4\n",
        "df.loc[df['label'] == 'sadness', 'labels'] = 0\n",
        "df.loc[df['label'] =='fear', 'labels'] = 3\n",
        "df.loc[df['label'] == 'love', 'labels'] = 2"
      ],
      "metadata": {
        "id": "msHAf6J0u6ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df['label']\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XXxyONCHud1g",
        "outputId": "75c0640d-a53f-4a44-faca-565b028fbb09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               tweet  labels\n",
              "0  jln jatibarupolisi tdk bs gertak gubernur eman...     1.0\n",
              "1  cewe lho kayak rasain sibuk jaga rasain sakit ...     1.0\n",
              "2  kepingin gudeg mbarek bu hj amad foto google s...     4.0\n",
              "3  jln jatibarubagian wilayah tn abangpengaturan ...     1.0\n",
              "4  sharing alam aja kemarin jam 1800 batalin tike...     4.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d73a67ce-1062-44f7-a9bc-2078ceaca17b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jln jatibarupolisi tdk bs gertak gubernur eman...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cewe lho kayak rasain sibuk jaga rasain sakit ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kepingin gudeg mbarek bu hj amad foto google s...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jln jatibarubagian wilayah tn abangpengaturan ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sharing alam aja kemarin jam 1800 batalin tike...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d73a67ce-1062-44f7-a9bc-2078ceaca17b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d73a67ce-1062-44f7-a9bc-2078ceaca17b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d73a67ce-1062-44f7-a9bc-2078ceaca17b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " df = df.rename(columns={\"labels\": \"tagging\"})"
      ],
      "metadata": {
        "id": "raXmUsOpv5Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mkR85GorwbVI",
        "outputId": "6aaf7cfc-6e3a-45e0-b799-496b4346c72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               tweet  tagging\n",
              "0  jln jatibarupolisi tdk bs gertak gubernur eman...      1.0\n",
              "1  cewe lho kayak rasain sibuk jaga rasain sakit ...      1.0\n",
              "2  kepingin gudeg mbarek bu hj amad foto google s...      4.0\n",
              "3  jln jatibarubagian wilayah tn abangpengaturan ...      1.0\n",
              "4  sharing alam aja kemarin jam 1800 batalin tike...      4.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-518241f2-6ffd-4f36-ac98-e0d4c370c421\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>tagging</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jln jatibarupolisi tdk bs gertak gubernur eman...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cewe lho kayak rasain sibuk jaga rasain sakit ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kepingin gudeg mbarek bu hj amad foto google s...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jln jatibarubagian wilayah tn abangpengaturan ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sharing alam aja kemarin jam 1800 batalin tike...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-518241f2-6ffd-4f36-ac98-e0d4c370c421')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-518241f2-6ffd-4f36-ac98-e0d4c370c421 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-518241f2-6ffd-4f36-ac98-e0d4c370c421');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tweet[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cuBW6N-NqEua",
        "outputId": "b0ccb080-f274-4031-a237-80945a929dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'kepingin gudeg mbarek bu hj amad foto google sengaja biar teman jg bayang bagi indah'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tagging.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKv3FfGuqJ8d",
        "outputId": "c5d95155-c3fb-4a4e-aa82-b4b041fed443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    1101\n",
              "4.0    1017\n",
              "0.0     997\n",
              "3.0     649\n",
              "2.0     637\n",
              "Name: tagging, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = df.tweet.values\n",
        "labels = df.tagging.values"
      ],
      "metadata": {
        "id": "c8UFJP2KqPm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(sentence):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()"
      ],
      "metadata": {
        "id": "49QR4MbyqVdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_short = []\n",
        "\n",
        "for index,sentences in enumerate(tweet):\n",
        "  sentences = preprocess_text(sentences)\n",
        "  temp_word = sentences.split()\n",
        "\n",
        "  length = len(temp_word)\n",
        "\n",
        "  if(length > 280):\n",
        "    #temp_word = temp_word[:129] + temp_word[length-383: length+1]\n",
        "    temp_word = temp_word[:280]\n",
        "    #temp_word = temp_word[length-512: length+1]\n",
        "\n",
        "  tweet_short.append(temp_word)"
      ],
      "metadata": {
        "id": "8cW26yEMqZuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_short[0][len(tweet_short[0]) - 20:len(tweet_short[0])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv8e6YOAqkv5",
        "outputId": "ef50a46d-f05e-4635-b893-975428c481ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jln',\n",
              " 'jatibarupolisi',\n",
              " 'tdk',\n",
              " 'bs',\n",
              " 'gertak',\n",
              " 'gubernur',\n",
              " 'emangny',\n",
              " 'polisi',\n",
              " 'tdk',\n",
              " 'pmbhasan',\n",
              " 'jgn',\n",
              " 'politik',\n",
              " 'atur',\n",
              " 'wilayahhak',\n",
              " 'gubernur',\n",
              " 'tn',\n",
              " 'abang',\n",
              " 'turun',\n",
              " 'temurunpelikperlu',\n",
              " 'sabar']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tweet_short[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hlfy7nsqvYW",
        "outputId": "0ef5dbc1-e40b-4e7c-e6fe-15985730f76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "model_name ='StevenLimcorn/indonesian-roberta-base-emotion-classifier'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "input_id = [tokenizer.encode(sent, add_special_tokens=True,max_length=280, truncation=True, padding='max_length') for sent in tweet]\n",
        "\n"
      ],
      "metadata": {
        "id": "sHsRj4C3q0Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_masks = [[float(i>0) for i in seq] for seq in input_id]"
      ],
      "metadata": {
        "id": "lg-37zR0rAWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_id[3][len(input_id[3])-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bs3fL0ErDQH",
        "outputId": "0c75d9b1-9caa-44d2-bfb2-a72641698fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split train and validation with 10:1 ratio\n",
        "train_inputs, validation_input, train_labels, validation_labels  = train_test_split(input_id, labels,random_state=32, test_size=0.1)\n",
        "# We only need to split attention mask into 2 variable, labels is not needed for this.\n",
        "train_masks, validation_masks, label_mask,_  = train_test_split(attention_masks, labels,random_state=32, test_size=0.1)\n",
        "\n",
        "# From the train_input, split again to train and prediction with 10:1 ratio\n",
        "train_inputs, prediction_input, train_labels, prediction_labels  = train_test_split(train_inputs, train_labels,random_state=3, test_size=0.2)\n",
        "train_masks, prediction_masks, _,_  = train_test_split(train_masks, label_mask,random_state=3, test_size=0.2)"
      ],
      "metadata": {
        "id": "h8O2SdMgrIdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_input)\n",
        "prediction_input = torch.tensor(prediction_input)\n",
        "\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
        "prediction_labels = torch.tensor(prediction_labels, dtype=torch.long)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "prediction_masks = torch.tensor(prediction_masks)"
      ],
      "metadata": {
        "id": "TnAN6ZHPrKYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "prediction_data = TensorDataset(prediction_input, prediction_masks, prediction_labels)\n",
        "prediction_sampler = RandomSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "7wR_Q9jyrpE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 5, # The number of output labels--2 for binary classification.\n",
        "    hidden_dropout_prob = 0.2, \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39SvBjEFrqae",
        "outputId": "86ed7f22-2f2f-46fe-a308-2a50d784fef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(model.named_parameters())\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xemkZKc5rs7l",
        "outputId": "bc82f878-f197-4be8-edaa-3dfe31c042c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier.dense.weight                                   (768, 768)\n",
            "classifier.dense.bias                                         (768,)\n",
            "classifier.out_proj.weight                                  (5, 768)\n",
            "classifier.out_proj.bias                                        (5,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps=1e-4, correct_bias=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzy2WeDkrwCT",
        "outputId": "a7c66280-9aac-4cbb-be2f-51d31f6ff992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 8\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "WiFiakHNrx2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "t4Q8nOUer0pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "ZFSeWsXgr27x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "train_loss_values = []\n",
        "eval_loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    train_loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None,\n",
        "                            labels=b_labels,\n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        eval_loss += loss.item()\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(validation_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    eval_loss_values.append(avg_eval_loss)\n",
        "\n",
        "        \n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\" validation loss : {:.2f}\".format(avg_eval_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ5fItQNr4zd",
        "outputId": "18ec51c2-fb96-4088-dba5-94ca6003e629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    396.    Elapsed: 0:00:35.\n",
            "  Batch    80  of    396.    Elapsed: 0:01:11.\n",
            "  Batch   120  of    396.    Elapsed: 0:01:46.\n",
            "  Batch   160  of    396.    Elapsed: 0:02:21.\n",
            "  Batch   200  of    396.    Elapsed: 0:02:57.\n",
            "  Batch   240  of    396.    Elapsed: 0:03:32.\n",
            "  Batch   280  of    396.    Elapsed: 0:04:07.\n",
            "  Batch   320  of    396.    Elapsed: 0:04:42.\n",
            "  Batch   360  of    396.    Elapsed: 0:05:18.\n",
            "\n",
            "  Average training loss: 1.12\n",
            "  Training epcoh took: 0:05:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.69\n",
            " validation loss : 0.85\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 2 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    396.    Elapsed: 0:00:35.\n",
            "  Batch    80  of    396.    Elapsed: 0:01:10.\n",
            "  Batch   120  of    396.    Elapsed: 0:01:46.\n",
            "  Batch   160  of    396.    Elapsed: 0:02:21.\n",
            "  Batch   200  of    396.    Elapsed: 0:02:56.\n",
            "  Batch   240  of    396.    Elapsed: 0:03:32.\n",
            "  Batch   280  of    396.    Elapsed: 0:04:07.\n",
            "  Batch   320  of    396.    Elapsed: 0:04:42.\n",
            "  Batch   360  of    396.    Elapsed: 0:05:17.\n",
            "\n",
            "  Average training loss: 0.80\n",
            "  Training epcoh took: 0:05:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            " validation loss : 0.88\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 3 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    396.    Elapsed: 0:00:35.\n",
            "  Batch    80  of    396.    Elapsed: 0:01:10.\n",
            "  Batch   120  of    396.    Elapsed: 0:01:46.\n",
            "  Batch   160  of    396.    Elapsed: 0:02:21.\n",
            "  Batch   200  of    396.    Elapsed: 0:02:56.\n",
            "  Batch   240  of    396.    Elapsed: 0:03:31.\n",
            "  Batch   280  of    396.    Elapsed: 0:04:07.\n",
            "  Batch   320  of    396.    Elapsed: 0:04:42.\n",
            "  Batch   360  of    396.    Elapsed: 0:05:17.\n",
            "\n",
            "  Average training loss: 0.67\n",
            "  Training epcoh took: 0:05:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.68\n",
            " validation loss : 0.87\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 4 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    396.    Elapsed: 0:00:35.\n",
            "  Batch    80  of    396.    Elapsed: 0:01:10.\n",
            "  Batch   120  of    396.    Elapsed: 0:01:46.\n",
            "  Batch   160  of    396.    Elapsed: 0:02:21.\n",
            "  Batch   200  of    396.    Elapsed: 0:02:56.\n",
            "  Batch   240  of    396.    Elapsed: 0:03:31.\n",
            "  Batch   280  of    396.    Elapsed: 0:04:07.\n",
            "  Batch   320  of    396.    Elapsed: 0:04:42.\n",
            "  Batch   360  of    396.    Elapsed: 0:05:17.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epcoh took: 0:05:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.71\n",
            " validation loss : 0.88\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 5 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    396.    Elapsed: 0:00:35.\n",
            "  Batch    80  of    396.    Elapsed: 0:01:10.\n",
            "  Batch   120  of    396.    Elapsed: 0:01:46.\n",
            "  Batch   160  of    396.    Elapsed: 0:02:21.\n",
            "  Batch   200  of    396.    Elapsed: 0:02:56.\n",
            "  Batch   240  of    396.    Elapsed: 0:03:31.\n",
            "  Batch   280  of    396.    Elapsed: 0:04:07.\n",
            "  Batch   320  of    396.    Elapsed: 0:04:42.\n",
            "  Batch   360  of    396.    Elapsed: 0:05:17.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epcoh took: 0:05:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.71\n",
            " validation loss : 0.92\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 6 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    396.    Elapsed: 0:00:35.\n",
            "  Batch    80  of    396.    Elapsed: 0:01:10.\n",
            "  Batch   120  of    396.    Elapsed: 0:01:46.\n",
            "  Batch   160  of    396.    Elapsed: 0:02:21.\n",
            "  Batch   200  of    396.    Elapsed: 0:02:56.\n",
            "  Batch   240  of    396.    Elapsed: 0:03:31.\n",
            "  Batch   280  of    396.    Elapsed: 0:04:06.\n",
            "  Batch   320  of    396.    Elapsed: 0:04:42.\n",
            "  Batch   360  of    396.    Elapsed: 0:05:17.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epcoh took: 0:05:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.72\n",
            " validation loss : 0.98\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 7 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    396.    Elapsed: 0:00:35.\n",
            "  Batch    80  of    396.    Elapsed: 0:01:11.\n",
            "  Batch   120  of    396.    Elapsed: 0:01:46.\n",
            "  Batch   160  of    396.    Elapsed: 0:02:21.\n",
            "  Batch   200  of    396.    Elapsed: 0:02:56.\n",
            "  Batch   240  of    396.    Elapsed: 0:03:32.\n",
            "  Batch   280  of    396.    Elapsed: 0:04:07.\n",
            "  Batch   320  of    396.    Elapsed: 0:04:42.\n",
            "  Batch   360  of    396.    Elapsed: 0:05:17.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epcoh took: 0:05:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.71\n",
            " validation loss : 1.03\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 8 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    396.    Elapsed: 0:00:35.\n",
            "  Batch    80  of    396.    Elapsed: 0:01:10.\n",
            "  Batch   120  of    396.    Elapsed: 0:01:46.\n",
            "  Batch   160  of    396.    Elapsed: 0:02:21.\n",
            "  Batch   200  of    396.    Elapsed: 0:02:56.\n",
            "  Batch   240  of    396.    Elapsed: 0:03:31.\n",
            "  Batch   280  of    396.    Elapsed: 0:04:07.\n",
            "  Batch   320  of    396.    Elapsed: 0:04:42.\n",
            "  Batch   360  of    396.    Elapsed: 0:05:17.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epcoh took: 0:05:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            " validation loss : 1.10\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(train_loss_values, label=\"train_loss\")\n",
        "plt.plot(eval_loss_values, label=\"eval_loss\")\n",
        "plt.title('Loss...')\n",
        "plt.legend(loc = 'best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "flLdczRpr7zT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "d42f21d2-0c29-4ebd-b554-1690a41d7180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on test set\n",
        "\n",
        "#print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "test_acc = 0.0\n",
        "test_f1 = 0.0\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labelstest\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ],
      "metadata": {
        "id": "iFkczbPyr98O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()"
      ],
      "metadata": {
        "id": "j4N6ykXqr_29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_true_labels = [item for sublist in true_labels for item in sublist]"
      ],
      "metadata": {
        "id": "bOcmHIobsCGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "\n",
        "print(classification_report(flat_predictions, flat_true_labels))"
      ],
      "metadata": {
        "id": "EXim_iiIsD2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aef4281-0e56-474f-daec-6e84b5bdaae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.58      0.59       178\n",
            "           1       0.68      0.83      0.75       166\n",
            "           2       0.80      0.82      0.81       109\n",
            "           3       0.76      0.70      0.73       131\n",
            "           4       0.82      0.73      0.77       208\n",
            "\n",
            "    accuracy                           0.72       792\n",
            "   macro avg       0.73      0.73      0.73       792\n",
            "weighted avg       0.73      0.72      0.72       792\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "cf_matrix = confusion_matrix(flat_predictions, flat_true_labels)\n",
        "plt.figure()\n",
        "sns.heatmap(cf_matrix, annot=True, fmt='d')"
      ],
      "metadata": {
        "id": "eCSFfQSWsHs5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "c3e8f520-fb39-41a0-a099-1061e8c4de4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd50b96eb10>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}